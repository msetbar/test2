# Stringified Metadata Semantic Settings Analysis <!-- omit in toc -->

## Table of Contents

- [Table of Contents](#table-of-contents)
- [Hypothesis](#hypothesis)
- [Approach](#approach)
- [Results](#results)
  - [Analysis](#analysis)
    - [General Results](#general-results)
    - [Chunk Overlap Analysis](#chunk-overlap-analysis)
    - [in\_top\_init Analysis](#in_top_init-analysis)
    - [Token Count Analysis](#token-count-analysis)
- [Future Ideas](#future-ideas)
- [Conclusion](#conclusion)

## Hypothesis

Why: We want to understand the impact of including the stringified metadata as a searchable field.
Going in we know that this will impact the semantic reranker results of ACS, which we evaluate through the initial retrieval metric results.
We first want to take a simplified approach of incorporating it as a concatenated single metadata field.
Note that currently the platform does not support including all the available document json fields as metadata.

We believe that including stringified metadata as a searchable field in the Azure Cognitive Search data index will result in more accurate chunks being retrieved during a semantic search query.

We will know we have succeeded when initial retrieval metrics (`init_rougeL_recall_median`, `in_top_init_%`) show a significant increase from the control (experiment on full dataset using a semantic search configuration that only considers text content as a prioritized field).

## Approach

This experimentation is scoped to using Azure Cognitive Search (ACS) as our vector database, as it has the out-of-the-box capability to search available metadata through the semantic search configuration.
There is a variety of metadata available on the documents in our dataset;
the fields currently included as metadata at index creation time are:

- `source`
- `seq_num`
- `document_id`
- `ArticleNumber`
- `Title`
- `UrlName`
- `articleUrl`
- `PublishStatus`
- `ArticleSummary`

[This article](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview) provides a good overview of semantic search in ACS and its components.
As part of the [semantic ranking process](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview#how-inputs-are-prepared), all fields included in the **prioritized_content_fields** parameter of the [semantic configuration](https://learn.microsoft.com/en-us/azure/search/semantic-how-to-query-request?tabs=rest%2Cportal-query#2---create-a-semantic-configuration) are stringified and concatenated in the order provided for each chunk entry in the initial results set.
In the control configuration, only the text `content` field of a chunk is included in the index, while in the other two experiment configurations, the stringified `metadata` field is included both before and after the textual content. 
For each document in the search result, the summarization model accepts up to 2,000 tokens, where a token is approximately 10 characters.
Note that strings that exceed the max token limit of 2000 are trimmed to meet this requirement, so text after that is ignored for the subsequent summarization and optional answer generation steps.
This 2000 token limit is split between the `title` (128 tokens), `keywords` (128 tokens), and `content` (remaining up til 2000) fields from the semantic config.
From here, the [text summarization step](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview#how-inputs-are-summarized) identifies the sentence most representative summary of the input string relative to the input query, returned as `@search.captions.text` in the response.
Note that the caption is verbatim from the document, not AI-generated text.
Semantic scoring is done at the caption level.

Note that changes to the semantic search configuration in this way mainly impacts the initial retrieval of chunks step in our experimentation workflow.
It will impact the downstream reranked retrieval as well given that reranked chunks are a subset of the initally-retrieved chunks, but the external reranker model doesn't have access to the metadata fields/caption generated by ACS semantic search.

Thehre are a few features of ACS that this experimentation doesn't focus on that may provide additional value to improve retrieval results and LLM text generation. 
The cog search internal reranker can use other fields such as title and keywords to improve the ranking.
Semantic ranking will return captions and (optionally) answers in the response which can be used to highlight the text for the user in the search results, and sending the semantic answer to the LLM could also improve the LLM quality.
However, this experiment only focuses on the impact of including the metadata as part of the searchable content.

As part of our analysis, we enabled debug output while making requests to ACS - note that this is currently only an option through the REST API.

## Results

### Analysis

#### General Results

This experiment is a direct comparison against an earlier cognitive search experiment with semantic search.
Due to this direct comparison, we will only analyze 3 experiments.
Note that this experimentation occurred prior to 11/15, when the update was made so that the token limit prior to the summarization step increased from 256 to 2000.
Some of the results/analysis still reference that limit; however, based on the results so far and the knowledge of how the semantic ranking process works, the overall conclusion should still be valid.

- Experiment 1 (Baseline) (43aa858c-a86b-4ff8-afcf-c3992f965527): Azure Cognitive Search with Semantic Search on the content field
  - Config: `care_cog-2023-10-25` - `index_definition.semantic_configuration_name` is defined, but not `index_definition.semantic_settings`, so `semantic_settings` is be created by default with the text content field as the only field in `prioritized_content_fields`.
- Experiment 2 (01592642-4936-41dc-a516-c296813c972d): Azure Cognitive Search with Semantic Search on the content and metadata field **(order=content,metadata)**
  - Config: `user_pb5253-2023-11-01` - `index_definition.semantic_settings.configuration[0].prioritized_fields.prioritized_content_fields[*] = [{'field_name': 'content'}, {'field_name': 'metadata'}]` 
- Experiment 3 (1f510891-700b-438b-a377-08f5bf460d69): Azure Cognitive Search with Semantic Search on the content and metadata field **(order=metadata,content)**
  - Config: `user_mn5253-2023-11-09` - `index_definition.semantic_settings.configuration[0].prioritized_fields.prioritized_content_fields[*] = [{'field_name': 'metadata'}, {'field_name': 'content'}]` 

Since the goal of this experiment is to try to maximize the results of search, the key metrics are the following:

- init_rougeL_recall_median
- in_top_init_%

| Experiment Name | Experiment Id | init_rougeL_recall_median | in_top_init_% |
| --- | --- | --- | --- |
| Experiment 1 (Baseline) | 43aa858c-a86b-4ff8-afcf-c3992f965527 | 0.804740957966764 | 0.637295081967213 |
| Experiment 2 | 01592642-4936-41dc-a516-c296813c972d | 0.80896751223854 | 0.64139344262295 |
| Experiment 3 | 1f510891-700b-438b-a377-08f5bf460d69 | 0.798734177215189 | 0.627049180327868 |

As we can see from the experiment results, there is only a slight improvements between baseline configuration (experiment 1) and the experiment 2.
We see that experiment 3 (where the **prioritized_content_fields** has metadata first and then content) performs worse than both the baseline experiment and experiment 2.
We theorize this was due to the token limitation (which forced truncation to 256 tokens) of semantic search, prior to the 11/15 update.
After further analysis, we observed that when searching for the top 20 chunks from Cognitive search, experiment 2, on average, shared about 17.87 chunks with the baseline.
The table below shows the shared chunks at different k-values.

**NOTE:** These values are only for the analysis of the baseline with experiment 2.

#### Chunk Overlap Analysis

| k | Avg. # Overlapping Chunks | Avg. % Overlapping Chunks |
| --- | --- | --- |
| 20 | 17.87 | 89.33% |
| 10 | 8.89 | 88.87% |
| 5 | 4.37 | 87.43% |

#### in_top_init Analysis

While there was only a marginal increase in the `in_top_init_%` metric and the `init_rougeL_recall_median`, we wanted to see if the rows in the baseline experiment that have a `in_top_init` greater than 0 in the baseline are all included in the results for experiment 2.

We found that there are a total of three questions that have an `in_top_init` from the baseline that are not included in experiment 2 and five questions from experiment 2 that have an `in_top_init` greater than 0 that have an `in_top_init` of 0 in the baseline.
In the table below, we can see that the questions, the numbers for `in_top_init` and `init_rougeL_recall`, and the experiment.
Interestingly, some of the experiments have the same `init_rougeL_recall` given `in_top_init` values that should suggest otherwise.
A few notable question ids where this is observed are:

- 10096
- 20125
- 20116

The full results can be seen in the table below.

|    id | bcss_question                                                                 |   in_top_init |   init_rougeL_recall | experiment              |
|------:|:------------------------------------------------------------------------------|--------------:|---------------------:|:------------------------|
| 10026 | What can I use FaST for?                                                      |             1 |             0.684211 | experiment 1 (baseline) |
| 10026 | What can I use FaST for?                                                      |             0 |             0.368421 | experiment 2            |
| 10096 | How do I identify if a customer has an Advanced Solutions product in Clarify? |             2 |             0.617647 | experiment 1 (baseline) |
| 10096 | How do I identify if a customer has an Advanced Solutions product in Clarify? |             0 |             0.617647 | experiment 2            |
| 10182 | What if a customer wants to file a complaint with the FCC?                    |             0 |             0.371429 | experiment 1 (baseline) |
| 10182 | What if a customer wants to file a complaint with the FCC?                    |             1 |             1        | experiment 2            |
| 20079 | How do I verify if the cutomer doesn't have the CTN?                          |             0 |             0.166052 | experiment 1 (baseline) |
| 20079 | How do I verify if the cutomer doesn't have the CTN?                          |             1 |             0.926199 | experiment 2            |
| 20116 | How to provision a grandfathered mobile share advantage plan                  |             0 |             0.463636 | experiment 1 (baseline) |
| 20116 | How to provision a grandfathered mobile share advantage plan                  |             1 |             0.518182 | experiment 2            |
| 20125 | How to cancel an unauthorized online order                                    |             0 |             0.911765 | experiment 1 (baseline) |
| 20125 | How to cancel an unauthorized online order                                    |             1 |             0.911765 | experiment 2            |
| 50018 | how to handle echat customers                                                 |             0 |             0.4375   | experiment 1 (baseline) |
| 50018 | how to handle echat customers                                                 |             1 |             1        | experiment 2            |
| 50026 | What are payment options?                                                     |             2 |             1        | experiment 1 (baseline) |
| 50026 | What are payment options?                                                     |             0 |             0.444444 | experiment 2            |

This document will not perform further analysis on the `rougeL` and `in_top_init` metric, but it may be worthwhile to investigate other cases where the `in_top_init` metric is 0 and the `init_rougeL_recall` is a high value.
In the case of 20125, we observed that the expected answer is a bit long, so there may exist another document that contains like information (skewing the results of `in_top_init`).

#### Token Count Analysis

We leveraged the `RecursiveCharacterTextSplitter` splitter with a chunk size of 1500 for all experiments.
Given cognitive search had a max input size of 256 tokens for the semantic search reranking process at experimentation time, we wanted to analyze how close we were to that limit for both the baseline configuration and the configuration with metadata (which should express larger input strings).
For our experiment, we took roughly 250 unique samples and measured how long the reranker input (the reranker input is the concatenated string of all the fields in the **prioritized_content_field** list).
Here are our results:

| Experiment | Median # of Tokens | 75th Percentile |
| --- | --- | --- |
| Baseline | 207 | 229 |
| Experiment 2 | 263 | 284 |

Since we know the input was trimmed at 256 tokens, we observe that over 50% of the samples in experiment 2 were trimmed (or lost some context) at experimentation time (though if this experiment were repeated with the current token limitations, this would not be the case).

## Future Ideas

- Leverage the title from the metadata as the content title
- Mapping the stringified document category metadata to the ACS `keywords` field
  - Evaluate the effect of this on retrieval
  - Implement query augmentation based on question category - would that + metadata improve retrieval
- Evaluate [semantic answers](https://learn.microsoft.com/en-us/azure/search/semantic-answers) feature - how does it compare to groundtruth answers and and does passing in these answers to LLM affect the output

## Conclusion

Due to the very small performance increase in the metrics, we reject the hypothesis that adding the stringified metadata will increase the performance of search.
We observed only a ~0.004 increase in the metrics for both `init_rougeL_recall` and `in_top_init` with an average chunk overlap ratio of about 90% for the 20 retrieved chunks.
While we observed that adding the stringified metadata to the **prioritized_content_fields** did not result in the performance improvements we were looking for, the infrastructure is now in place to perform future experimentation around fields in the semantic search settings and metadata usage.
It may be a worthwhile experiment/spike to observe how the results perform with a subset of the metadata (by creating fields for the most informative metadata keys, like `ArticleSummary` or `Title`).
This would require updates to the DomainServices code to properly extract and propagate the available document metadata to the semantic settings fields in ACS.
